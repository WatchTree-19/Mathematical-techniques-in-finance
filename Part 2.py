# -*- coding: utf-8 -*-
"""nlp_project_part2_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wTkQBQBld-QCjmtD2XB5Q98AHWVRhG08

# Project part 2
---
This document stores the codes for predicting sentiment label and calculating sentiment scores for each text.

- First using pre-trained model from hugging face named `j-hartmann/emotion-english-distilroberta-base` to label the text data.

- The model is a fine-tuned checkpoint of `DistilRoBERTa-base`. It uses tens of thousands of data from social media to train the model. For more information about this model, check https://huggingface.co/distilroberta-base and https://huggingface.co/j-hartmann/emotion-english-distilroberta-base.
"""

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/nlp_data/'

import numpy as np
import pandas as pd



"""# Deep learning model for sentiment analysis"""

! pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer
# Create class for data preparation
class SimpleDataset:
    def __init__(self, tokenized_texts):
        self.tokenized_texts = tokenized_texts
    
    def __len__(self):
        return len(self.tokenized_texts["input_ids"])
    
    def __getitem__(self, idx):
        return {k: v[idx] for k, v in self.tokenized_texts.items()}

# load tokenizer and model, create trainer
model_name = "j-hartmann/emotion-english-distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
trainer = Trainer(model=model)

def Get_senti_score(pred_df):    
    pred_texts = pred_df['clean_text'].astype(str).tolist()
    indexs = pred_df.index
    # Tokenize texts and create prediction data set
    tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)
    pred_dataset = SimpleDataset(tokenized_texts)

    # run prediction
    predictions = trainer.predict(pred_dataset)
    
    # Transform predictions to labels
    preds = predictions.predictions.argmax(-1)
    labels = pd.Series(preds).map(model.config.id2label)
    scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)
    # scores raw
    temp = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True))
    
    # work in progress
    # container
    anger = []
    disgust = []
    fear = []
    joy = []
    neutral = []
    sadness = []
    surprise = []

    # extract scores (as many entries as exist in pred_texts)
    for i in range(len(pred_texts)):
        anger.append(temp[i][0])
        disgust.append(temp[i][1])
        fear.append(temp[i][2])
        joy.append(temp[i][3])
        neutral.append(temp[i][4])
        sadness.append(temp[i][5])
        surprise.append(temp[i][6])

    # return DataFrame with texts, predictions, labels, and scores
    df = pd.DataFrame(list(zip(pred_texts,preds,labels,scores,  anger, disgust, fear, joy, neutral, sadness, surprise)), columns=['text','pred','label','score', 'anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'])
    df.index = indexs
    return df

# create predict text, load the data again
tweetData = {}
names = ['AAPL']
pd.read_csv('AAPL_processed.csv',parse_dates=['Est_date'], index_col = 'Est_date')
#for name in names:
    #tweetData[name] = pd.read_csv(name+'_processed.csv',parse_dates=['Est_date'], index_col = 'Est_date')

tweetData

other_aapl = tweetData['AAPL'].copy()

other_aapl['clean_text'] = other_aapl['body_sw_stem']

other_aapl = other_aapl.drop(columns = ['body_sw_stem'])

other_sub = other_aapl[1:10000]

other_sub_pred = Get_senti_score(other_sub)

other_sub_pred.to_csv(path+'other_scored.csv')

"""## Data split

We need to split the tweet data since it is to large and session will get crashed.

We split the dataset by seasons and feed each subset into the trainer then combine them all and export to csv file.

Apple data
"""

# time interval by seasons
# 2021-01-01 and 2021-04-01
aapl_s1 = tweetData['AAPL'].loc['2021-01-01': '2021-03-31']
aapl_s2 = tweetData['AAPL'].loc['2021-04-01': '2021-06-30']
aapl_s3 = tweetData['AAPL'].loc['2021-07-01': '2021-09-30']
aapl_s4 = tweetData['AAPL'].loc['2021-10-01': ]

aapl_s1_pred = Get_senti_score(aapl_s1)

aapl_s2_pred = Get_senti_score(aapl_s2)

aapl_s3_pred = Get_senti_score(aapl_s3)

aapl_s4_pred = Get_senti_score(aapl_s4)

aapl_rbind_all = pd.concat([aapl_s1_pred,aapl_s2_pred,aapl_s3_pred, aapl_s4_pred],           # Rbind DataFrames
                           ignore_index = False,
                           sort = False)

len(aapl_rbind_all)

aapl_rbind_all.head(5)

# export to csv file
aapl_rbind_all.to_csv(path+'AAPL_scored.csv')

"""TSLA data

Unfinished
"""

# time interval by seasons
# 2021-01-01 and 2021-04-01
tsla_s1 = tweetData['TSLA'].loc['2021-01-01': '2021-02-15']
tsla_s2 = tweetData['TSLA'].loc['2021-02-16': '2021-03-31']
tsla_s3 = tweetData['TSLA'].loc['2021-04-01': '2021-05-15']
tsla_s4 = tweetData['TSLA'].loc['2021-05-16': '2021-06-30']
tsla_s5 = tweetData['TSLA'].loc['2021-07-01': '2021-08-31']
tsla_s6 = tweetData['TSLA'].loc['2021-09-01': '2021-10-31']
tsla_s7 = tweetData['TSLA'].loc['2021-11-01': '2021-12-31']
tsla_s8 = tweetData['TSLA'].loc['2022-01-01': '2022-02-15']
tsla_s9 = tweetData['TSLA'].loc['2022-02-16': ]

len(tweetData['TSLA'])

len(tweetData['TSLA'])==len(tsla_s1)+len(tsla_s2)+len(tsla_s3)+len(tsla_s4)+len(tsla_s5)+len(tsla_s6)+len(tsla_s7)+len(tsla_s8)+len(tsla_s9)

len(tsla_s5)

tsla_s1_pred = Get_senti_score(tsla_s1)

tsla_s2_pred = Get_senti_score(tsla_s2)

tsla_s3_pred = Get_senti_score(tsla_s3)

tsla_s4_pred = Get_senti_score(tsla_s4)

tsla_s5_pred = Get_senti_score(tsla_s5)

tsla_s6_pred = Get_senti_score(tsla_s6)

tsla_s7_pred = Get_senti_score(tsla_s7)

tsla_s8_pred = Get_senti_score(tsla_s8)

tsla_s9_pred = Get_senti_score(tsla_s9)

tsla_rbind_all = pd.concat([tsla_s1_pred, tsla_s2_pred,tsla_s3_pred,tsla_s4_pred, tsla_s5_pred, tsla_s6_pred, 
                            tsla_s7_pred,tsla_s8_pred,tsla_s9_pred],           # Rbind DataFrames
                           ignore_index = False,
                           sort = False)

len(tsla_rbind_all)

tsla_rbind_all.head(5)

# export to csv file
tsla_rbind_all.to_csv(path+'TSLA_scored.csv')

"""FB data"""

# time interval by seasons
fb_s1 = tweetData['FB'].loc['2021-01-01': '2021-03-31']
fb_s2 = tweetData['FB'].loc['2021-04-01': '2021-06-30']
fb_s3 = tweetData['FB'].loc['2021-07-01': '2021-09-30']
fb_s4 = tweetData['FB'].loc['2021-10-01': ]

len(tweetData['FB'])==len(fb_s1)+len(fb_s2)+len(fb_s3)+len(fb_s4)

len(fb_s4)

fb_s1_pred = Get_senti_score(fb_s1)

fb_s2_pred = Get_senti_score(fb_s2)

fb_s3_pred = Get_senti_score(fb_s3)

fb_s4_pred = Get_senti_score(fb_s4)

fb_rbind_all = pd.concat([fb_s1_pred, fb_s2_pred,fb_s3_pred,fb_s4_pred],           # Rbind DataFrames
                           ignore_index = False,
                           sort = False)

len(fb_rbind_all)

fb_rbind_all.head(5)

# export to csv file
fb_rbind_all.to_csv(path+'FB_scored.csv')

"""Amazon data"""

# time interval by seasons
# 2021-01-01 and 2021-04-01
amzn_s1 = tweetData['AMZN'].loc['2021-01-01': '2021-03-31']
amzn_s2 = tweetData['AMZN'].loc['2021-04-01': '2021-06-30']
amzn_s3 = tweetData['AMZN'].loc['2021-07-01': '2021-09-30']
amzn_s4 = tweetData['AMZN'].loc['2021-10-01': ]

len(amzn_s2)

len(tweetData['AMZN'])==len(amzn_s1)+len(amzn_s2)+len(amzn_s3)+len(amzn_s4)

amzn_s1_pred = Get_senti_score(amzn_s1)

amzn_s2_pred = Get_senti_score(amzn_s2)

amzn_s3_pred = Get_senti_score(amzn_s3)

amzn_s4_pred = Get_senti_score(amzn_s4)

amzn_rbind_all = pd.concat([amzn_s1_pred, amzn_s2_pred,amzn_s3_pred,amzn_s4_pred],           # Rbind DataFrames
                           ignore_index = False,
                           sort = False)

len(amzn_rbind_all)

amzn_rbind_all.head(5)

# export to csv file
amzn_rbind_all.to_csv(path+'AMZN_scored.csv')

"""NVDA data"""

# time interval by seasons
# 2021-01-01 and 2021-04-01
nvda_s1 = tweetData['NVDA'].loc['2021-01-01': '2021-03-31']
nvda_s2 = tweetData['NVDA'].loc['2021-04-01': '2021-06-30']
nvda_s3 = tweetData['NVDA'].loc['2021-07-01': '2021-09-30']
nvda_s4 = tweetData['NVDA'].loc['2021-10-01': ]

len(tweetData['NVDA'])==len(nvda_s1)+len(nvda_s2)+len(nvda_s3)+len(nvda_s4)

nvda_s1_pred = Get_senti_score(nvda_s1)

nvda_s2_pred = Get_senti_score(nvda_s2)

nvda_s3_pred = Get_senti_score(nvda_s3)

nvda_s4_pred = Get_senti_score(nvda_s4)

nvda_rbind_all = pd.concat([nvda_s1_pred, nvda_s2_pred,nvda_s3_pred,nvda_s4_pred],           # Rbind DataFrames
                           ignore_index = False,
                           sort = False)

len(nvda_rbind_all)

len(tweetData['NVDA'])

nvda_rbind_all.head(5)

# export to csv file
nvda_rbind_all.to_csv(path+'NVDA_scored.csv')
